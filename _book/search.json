[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feture Engineering in R",
    "section": "",
    "text": "Preface\nThis material is from the DataCamp course Feature Engineering in R by Jorge Zazueta.\nCourse Description: In this course, you’ll learn about feature engineering, which is at the heart of many times of machine learning models. As the performance of any model is a direct consequence of the features it’s fed, feature engineering places domain knowledge at the center of the process. You’ll become acquainted with principles of sound feature engineering, helping to reduce the number of variables where possible, making learning algorithms run faster, improving interpretability, and preventing overfitting.\nYou will learn how to implement feature engineering techniques using the R tidymodels framework, emphasizing the recipe package that will allow you to create, extract, transform, and select the best features for your model.\nWhen faced with a new dataset, you will be able to identify and select relevant features and disregard non-informative ones to make your model run faster without sacrificing accuracy. You will also become comfortable applying transformations and creating new features to make your models more efficient, interpretable, and accurate\nReminder to self: each *.qmd file contains one and only one chapter, and a chapter is defined by the first-level heading #."
  },
  {
    "objectID": "01-FE.html#what-is-feature-engineering---video",
    "href": "01-FE.html#what-is-feature-engineering---video",
    "title": "1  Introducing Feature Engineering",
    "section": "What is feature engineering? - (video)",
    "text": "What is feature engineering? - (video)"
  },
  {
    "objectID": "01-FE.html#a-tentative-model",
    "href": "01-FE.html#a-tentative-model",
    "title": "1  Introducing Feature Engineering",
    "section": "1.1 A Tentative Model",
    "text": "1.1 A Tentative Model\nYou are handed a data set with measures of the gravitational force between two bodies at different distances and are challenged to build a simple model to predict such force given a specific distance. Initially, you want to stick to simple linear regression. The data consist of 120 pairs of distance and force, and is loaded for you as newton."
  },
  {
    "objectID": "01-FE.html#instructions",
    "href": "01-FE.html#instructions",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nBuild a linear model for the newton data using the linear model from base R function and assign it to lr_force.\nCreate a new data frame df by binding the prediction values to the original newton data.\nGenerate a scatterplot of force versus distance using ggplot().\nAdd a regression line to the scatterplot with the fitted values.\n\n\n# Build a linear model for the newton the data and assign it to lr_force\nlr_force &lt;- lm(force ~ distance, data = newton)\n\n# Create a new data frame by binding the prediction values to the original data\ndf &lt;- newton %&gt;% bind_cols(lr_pred = predict(lr_force))\n\n# Generate a scatterplot of force vs. distance\ndf %&gt;%\n  ggplot(aes(x = distance, y = force)) +\n  geom_point() +\n# Add a regression line with the fitted values\n  geom_line(aes(y = lr_pred), color = \"blue\", lwd = .75) +\n  ggtitle(\"Linear regression of force vs. distance\") +\n  theme_classic()"
  },
  {
    "objectID": "01-FE.html#manually-engineering-a-feature",
    "href": "01-FE.html#manually-engineering-a-feature",
    "title": "1  Introducing Feature Engineering",
    "section": "1.2 Manually engineering a feature",
    "text": "1.2 Manually engineering a feature\nAfter doing some research with your team, you recall that the gravitational force of attraction between two bodies obeys Newton’s formula: \\[F = G\\frac{m_1m_2}{r^2}\\] You can’t use the formula directly because the masses are unknown, but you can fit a regression model of force as a function of inv_square_distance. The augmented dataset df you built in the previous exercise has been loaded for you."
  },
  {
    "objectID": "01-FE.html#instructions-1",
    "href": "01-FE.html#instructions-1",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a new variable inv_square_distance defined as the reciprocal of the squared distance and add it to the df data frame.\n\n*Build a simple regression model using lm() of force versus inv_square_distance and save it as lr_force_2.\n*Bind your predictions to df_inverse.\n\n# Create a new variable inv_square_distance\ndf_inverse &lt;- df %&gt;% mutate(inv_square_distance = 1/distance^2)\n\n# Build a simple regression model\nlr_force_2 &lt;- lm(force ~ inv_square_distance, data = df_inverse)\n\n# Bind your predictions to df_inverse\ndf_inverse &lt;- df_inverse %&gt;% bind_cols(lr2_pred = predict(lr_force_2))\n\ndf_inverse %&gt;% ggplot(aes(x = distance, y = force)) +\n  geom_point() +\n  geom_line(aes(y = lr2_pred), col = \"blue\", lwd = .75) +\n  ggtitle(\"Linear regression of force vs. inv_square_distance\") +\n  theme_classic()"
  },
  {
    "objectID": "01-FE.html#creating-new-features-using-domain-knowledge---video",
    "href": "01-FE.html#creating-new-features-using-domain-knowledge---video",
    "title": "1  Introducing Feature Engineering",
    "section": "Creating new features using domain knowledge - (video)",
    "text": "Creating new features using domain knowledge - (video)"
  },
  {
    "objectID": "01-FE.html#setting-up-your-data-for-analysis",
    "href": "01-FE.html#setting-up-your-data-for-analysis",
    "title": "1  Introducing Feature Engineering",
    "section": "1.3 Setting up your data for analysis",
    "text": "1.3 Setting up your data for analysis\nou will look at a version of the nycflights13 dataset, loaded as flights. It contains information on flights departing from New York City. You are interested in predicting whether or not they will arrive late to their destination, but first, you need to set up the data for analysis.\nAfter discussing our model goals with a team of experts, you selected the following variables for your model: flight, sched_dep_time, dep_delay, sched_arr_time, carrier, origin, dest, distance, date, arrival.\nYou will also mutate() the date using as.Date() and convert character type variables to factors.\nLastly, you will split the data into train and test datasets."
  },
  {
    "objectID": "01-FE.html#instructions-2",
    "href": "01-FE.html#instructions-2",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nTransform all character-type variables to factors.\nSplit the flights data into test and train sets.\n\n\nset.seed(24601)\nflights &lt;- flights %&gt;%\n  slice_sample(n= 18182) \n#str(flights)\n\n\n\nCode\nflights &lt;- flights %&gt;%\n  mutate(date = make_date(year, month, day)) %&gt;%\n  mutate(arr = sched_arr_time - arr_time) %&gt;%\n  mutate(arrival = ifelse(arr &lt;= -50, \"late\", \"on-time\")) %&gt;%\n  select(flight, sched_dep_time, dep_delay, sched_arr_time, carrier, origin, dest, distance, date, arrival) %&gt;%\n  mutate(date = as.Date(date), across(where(is.character), as.factor))\n\n\n\n# Split the flights data into test and train sets\nset.seed(246)\nsplit &lt;- flights %&gt;% initial_split(prop = 3/4, strata = arrival)\ntest &lt;- testing(split)\ntrain &lt;- training(split)\n\ntest %&gt;% select(arrival) %&gt;% table() %&gt;% prop.table()\n\narrival\n     late   on-time \n0.1640236 0.8359764 \n\ntrain %&gt;% select(arrival) %&gt;% table() %&gt;% prop.table()\n\narrival\n     late   on-time \n0.1669435 0.8330565"
  },
  {
    "objectID": "01-FE.html#building-a-workflow",
    "href": "01-FE.html#building-a-workflow",
    "title": "1  Introducing Feature Engineering",
    "section": "1.4 Building a workflow",
    "text": "1.4 Building a workflow\nWith your data ready for analysis, you will declare a logistic_model() to predict whether or not they will arrive late.\nYou assign the role of “ID” to the flight variable to keep it as a reference for analysis and debugging. From the date variable, you will create new features to explicitly model the effect of holidays and represent factors as dummy variables.\nBundling your model and recipe() together using workflow() will help ensure that subsequent fittings or predictions will implement consistent feature engineering steps."
  },
  {
    "objectID": "01-FE.html#instructions-3",
    "href": "01-FE.html#instructions-3",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nAssign an “ID” role to flight.\nBundle the model and the recipe into a workflow object.\nFit lr_workflow to the test data.\nTidy the fitted workflow.\n\n\nlr_model &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# Assign an \"ID\" role to flight\nlr_recipe &lt;- recipe(arrival ~., data = train) %&gt;% update_role(flight, new_role = \"ID\") %&gt;%\n  step_holiday(date, holidays = timeDate::listHolidays(\"US\")) %&gt;% step_dummy(all_nominal_predictors())\n\n# Bundle the model and the recipe into a workflow object\nlr_workflow &lt;- workflow() %&gt;% add_model(lr_model) %&gt;% add_recipe(lr_recipe)\nlr_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_holiday()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n# Fit lr_workflow workflow to the test data  \nlr_fit &lt;- lr_workflow %&gt;% fit(data = test)\n\n# Tidy the fitted workflow  \ntidy(lr_fit)\n\n# A tibble: 138 × 5\n   term                           estimate   std.error statistic   p.value\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                  -34.5      3956.        -0.00873 9.93e-  1\n 2 sched_dep_time                -0.000572    0.000162  -3.53    4.10e-  4\n 3 dep_delay                     -0.0365      0.00165  -22.1     4.69e-108\n 4 sched_arr_time                 0.000502    0.000149   3.37    7.52e-  4\n 5 distance                       0.0307      0.0142     2.16    3.07e-  2\n 6 date                          -0.000262    0.000503  -0.520   6.03e-  1\n 7 date_USChristmasDay            0.935       1.24       0.752   4.52e-  1\n 8 date_USColumbusDay            15.2       959.         0.0159  9.87e-  1\n 9 date_USCPulaskisBirthday      15.9      1687.         0.00941 9.92e-  1\n10 date_USDecorationMemorialDay   0.612       1.12       0.549   5.83e-  1\n# ℹ 128 more rows"
  },
  {
    "objectID": "01-FE.html#increasing-the-information-content-of-raw-data--video",
    "href": "01-FE.html#increasing-the-information-content-of-raw-data--video",
    "title": "1  Introducing Feature Engineering",
    "section": "Increasing the information content of raw data -(video)",
    "text": "Increasing the information content of raw data -(video)"
  },
  {
    "objectID": "01-FE.html#identifying-missing-values",
    "href": "01-FE.html#identifying-missing-values",
    "title": "1  Introducing Feature Engineering",
    "section": "1.5 Identifying missing values",
    "text": "1.5 Identifying missing values\nAttrition is a critical issue for corporations, as losing an employee implies not only the cost of recruiting and training a new one, but constitutes a loss in tacit knowledge and culture that is hard to recover.\nThe attrition dataset has information on employee attrition including Age, WorkLifeBalance, DistanceFromHome, StockOptionLevel, and 27 others. Before continuing with your analysis, you want to detect any missing variables.\nThe package naniar and the attrition dataset are already loaded for you.\n\nlibrary(naniar)\nset.seed(345)\nattrition &lt;- attrition %&gt;%\n  mutate(BusinessTravel = replace(BusinessTravel, sample(row_number(), size = ceiling(.18*n()), replace = FALSE), NA), \n                                  DistanceFromHome = replace(DistanceFromHome, sample(row_number(), size = ceiling(.10*n()), replace = FALSE), NA),\n                                  StockOptionLevel = replace(StockOptionLevel, sample(row_number(), size = ceiling(.15*n()), replace = FALSE), NA),\n                                  WorkLifeBalance = replace(WorkLifeBalance, sample(row_number(), size = ceiling(.10*n()), replace = FALSE), NA))\n\nattrition$...1 &lt;- 1:1470\nattrition &lt;- attrition %&gt;%\n  relocate(...1, .before = Age)\n\nset.seed(89)\nattrition_split &lt;- initial_split(attrition, prop = .75, strata = Attrition)\ntrain &lt;- training(attrition_split)\ntest &lt;- testing(attrition_split)"
  },
  {
    "objectID": "01-FE.html#instructions-4",
    "href": "01-FE.html#instructions-4",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nExplore missing data visually to identify missing values on the attrition data.\n\n\n# Explore missing data on the attrition dataset\nvis_miss(attrition)\n\n\n\n\n\n\n\n\n\nSelect the variables with missing values and visualize only those.\n\n\n# Select the variables with missing values and rerun the analysis on those variables.\nattrition %&gt;% \n  select(\"BusinessTravel\", \"DistanceFromHome\",\n         \"StockOptionLevel\", \"WorkLifeBalance\") %&gt;%\n  vis_miss()"
  },
  {
    "objectID": "01-FE.html#imputing-missing-values-and-creating-dummy-variables",
    "href": "01-FE.html#imputing-missing-values-and-creating-dummy-variables",
    "title": "1  Introducing Feature Engineering",
    "section": "1.6 Imputing missing values and creating dummy variables",
    "text": "1.6 Imputing missing values and creating dummy variables\nAfter detecting missing values in the attrition dataset and determining that they are missing completely at random (MCAR), you decide to use K Nearest Neighbors (KNN) imputation. While configuring your feature engineering recipe, you decide to create dummy variables for all your nominal variables and update the role of the ...1 variable to “ID” so you can keep it in the dataset for reference, without affecting your model."
  },
  {
    "objectID": "01-FE.html#instructions-5",
    "href": "01-FE.html#instructions-5",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nUpdate the role of ...1 to “ID”.\nImpute values to all predictors where data are missing.\nCreate dummy variables for all nominal predictors.\n\n\nr_model &lt;- logistic_reg()\n\nlr_recipe &lt;- \n  recipe(Attrition ~., data = train) %&gt;%\n\n# Update the role of \"...1\" to \"ID\"\n  update_role(...1, new_role = \"ID\" ) %&gt;%\n\n# Impute values to all predictors where data are missing\n  step_impute_knn(all_nominal_predictors()) %&gt;%\n\n# Create dummy variables for all nominal predictors\n  step_dummy(all_nominal_predictors())\n\nlr_recipe"
  },
  {
    "objectID": "01-FE.html#fitting-and-assessing-the-model",
    "href": "01-FE.html#fitting-and-assessing-the-model",
    "title": "1  Introducing Feature Engineering",
    "section": "1.7 Fitting and assessing the model",
    "text": "1.7 Fitting and assessing the model\nNow that you have addressed missing values and created dummy variables, it is time to assess your model’s performance!\nThe attrition dataset, along with the test and train splits, the lr_recipe and your declared logistic_model() are all loaded for you."
  },
  {
    "objectID": "01-FE.html#instructions-6",
    "href": "01-FE.html#instructions-6",
    "title": "1  Introducing Feature Engineering",
    "section": "Instructions",
    "text": "Instructions\n\nBundle model and recipe in workflow.\nFit workflow to the train data.\nGenerate an augmented data frame for performance assessment.\n\n\n# Bundle model and recipe in workflow\nlr_workflow &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe)\n\n# Fit workflow to the train data\nlr_fit &lt;- fit(lr_workflow, data = train)\n\n# Generate an augmented data frame for performance assessment\nlr_aug &lt;- lr_fit %&gt;% augment(test)\n\nlr_aug %&gt;% roc_curve(truth = Attrition, .pred_No) %&gt;% autoplot()\n\n\n\n\n\n\n\nbind_rows(lr_aug %&gt;% roc_auc(truth = Attrition, .pred_No),          \n          lr_aug %&gt;% accuracy(truth = Attrition, .pred_class))\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.840\n2 accuracy binary         0.865"
  },
  {
    "objectID": "01-FE.html#predicting-hotel-bookings",
    "href": "01-FE.html#predicting-hotel-bookings",
    "title": "1  Introducing Feature Engineering",
    "section": "1.8 Predicting Hotel Bookings",
    "text": "1.8 Predicting Hotel Bookings\nYou just got a job at a hospitality research company, and your first task is to build a model that predicts whether or not a hotel stay will include children. To train your model, you will rely on a modified version of the hotel booking demands dataset by Antonio, Almeida, and Nunes (2019). You are restricting your data to the following features:\n\nfeatures &lt;- c('adults', \n              'children', 'meal',\n              'reserved_room_type', \n              'customer_type', \n              'arrival_date')\n\n\n\nCode\nlibrary(lubridate)\nhotel_booking &lt;- read_csv(\"./DATA/hotel_booking.csv\")\nhotel_booking &lt;- hotel_booking %&gt;%\n  mutate(month_no = match(arrival_date_month, month.name)) %&gt;%\n  mutate(arrival_date = make_date(arrival_date_year, month_no, arrival_date_day_of_month)) %&gt;%\n  mutate(children = ifelse( children == 0, \"none\", \"children\"))\n\nset.seed(90)\nhotel_booking %&gt;%\n  sample_n(size = 50000) %&gt;%\n  select(\"hotel\", 'adults', \n              'children', 'meal',\n              'reserved_room_type', \n              'customer_type', \n              'arrival_date') %&gt;%\n  mutate_if(is.character, as.factor) -&gt; hotelsOL\n\nhotelsOL$...1 &lt;- 1:50000\nhotelsOL &lt;- hotelsOL %&gt;%\n  relocate(...1, .before = hotel)\n\n\n\nset.seed(4)\nhotel_split &lt;- initial_split(hotelsOL, prop = .75, strata = hotel)\n\ntrain &lt;- training(hotel_split)\ntest &lt;- testing(hotel_split)\n\nlr_model &lt;- logistic_reg()\n\nThe data has been loaded for you as hotels, along with its corresponding test and train splits, and the model has been declared as lr_model &lt;- logistic_reg().\nYou will assess model performance by accuracy and area under the ROC curve or AUC.\n##Instructions {-}\n\nGenerate “day of the week”, “week” and “month” features.\nCreate dummy variables for all nominal predictors.\n\n\nlr_recipe &lt;- \n  recipe(children ~., data = train) %&gt;%\n# Generate \"day of the week\", \"week\" and \"month\" features\n\n  step_date(arrival_date, features = c(\"dow\", \"week\", \"month\")) %&gt;%\n\n# Create dummy variables for all nominal predictors\n  step_dummy(all_nominal_predictors())\n\n\nBundle your model and recipe in a workflow().\nFit the workflow to the training data.\n\n\n# Bundle your model and recipe in a workflow\nlr_workflow &lt;-workflow() %&gt;% add_model(lr_model) %&gt;% add_recipe(lr_recipe)\n\n# Fit the workflow to the training data\n\nlr_fit &lt;-  lr_workflow %&gt;% \n  fit(data = train)\nlr_aug &lt;- lr_fit %&gt;% \n  augment(test)\n\nbind_rows(roc_auc(lr_aug,truth = children, .pred_children),accuracy(lr_aug,truth = children, .pred_class))\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.853\n2 accuracy binary         0.948"
  },
  {
    "objectID": "02-FE.html#why-transform-existing-features--video",
    "href": "02-FE.html#why-transform-existing-features--video",
    "title": "2  Transforming Features",
    "section": "Why transform existing features? -(video)",
    "text": "Why transform existing features? -(video)"
  },
  {
    "objectID": "02-FE.html#glancing-at-your-data",
    "href": "02-FE.html#glancing-at-your-data",
    "title": "2  Transforming Features",
    "section": "2.1 Glancing at your data",
    "text": "2.1 Glancing at your data\n\n\nCode\nlibrary(modeldata)\nattrition_num &lt;- attrition %&gt;%\n  select(Attrition, Age, DailyRate, DistanceFromHome, HourlyRate, JobLevel, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\nThe dataset attrition_num has been loaded for you. Use the console to write code as necessary to answer the following questions: (1) How many observations are in the dataset? (2) How many numeric predictors? and (3) How many factor variables?\n\nThere are 1,470 observations, 16 numeric predictors and one factor variable.\nThere are 1,470 observations, 17 numeric predictors and one factor variable.\nThere are 1,490 observations, 0 numeric predictors and two factor variables."
  },
  {
    "objectID": "02-FE.html#normalizing-and-log-transforming",
    "href": "02-FE.html#normalizing-and-log-transforming",
    "title": "2  Transforming Features",
    "section": "2.2 Normalizing and log-transforming",
    "text": "2.2 Normalizing and log-transforming\nYou are handed a dataset, attrition_num with numerical data about employees who left the company. Features include Age, DistanceFromHome, and MonthlyRate.\nYou want to use this data to build a model that can predict if an employee is likely to stay, denoted by Attrition, a binary variable coded as a factor. In preparation for modeling, you want to reduce possible skewness and prevent some variables from outweighing others due to variations in scale.\nThe attrition_num data and the train and test splits are loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions",
    "href": "02-FE.html#instructions",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nNormalize all numeric predictors.\nLog-transform all numeric features, with an offset of one.\n\n\nattrition_split &lt;- initial_split(attrition_num, prop = 3/4, strata = Attrition)\n\ntrain &lt;- training(attrition_split)\ntest &lt;- testing(attrition_split)\n\n\nlr_model &lt;- logistic_reg()\n\nlr_recipe &lt;- \n  recipe(Attrition~., data = train) %&gt;%\n\n# Normalize all numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n\n# Log-transform all numeric features, with an offset of one\n  step_log(all_nominal_predictors(), offset = 1)\n\nlr_workflow &lt;- \n  workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe)\n\nlr_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_log()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "02-FE.html#fit-and-augment",
    "href": "02-FE.html#fit-and-augment",
    "title": "2  Transforming Features",
    "section": "2.3 Fit and augment",
    "text": "2.3 Fit and augment\nWith your lr_workflow ready to go, you can fit it to the test data to make predictions.\nFor model assessment, it is convenient to augment your fitted object by adding predictions and probabilities using augment()."
  },
  {
    "objectID": "02-FE.html#instructions-1",
    "href": "02-FE.html#instructions-1",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nFit the workflow to the train data.\nAugment the lr_fit object using the test data to get it ready for assessment.\n\n\n# Fit the workflow to the train data\nlr_fit &lt;- \n  fit(lr_workflow, data = train)\n\n# Augment the lr_fit object\nlr_aug &lt;-\n  augment(lr_fit, new_data = test)\n\nlr_aug\n\n# A tibble: 369 × 20\n   .pred_class .pred_No .pred_Yes Attrition   Age DailyRate DistanceFromHome\n * &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;     &lt;int&gt;            &lt;int&gt;\n 1 No             0.795    0.205  No           33      1392                3\n 2 No             0.665    0.335  No           27       591                2\n 3 No             0.844    0.156  No           29       153               15\n 4 No             0.546    0.454  No           31       670               26\n 5 No             0.940    0.0596 No           32       334                5\n 6 No             0.787    0.213  No           22      1123               16\n 7 No             0.876    0.124  No           24       673               11\n 8 No             0.950    0.0502 No           34       419                7\n 9 No             0.675    0.325  Yes          32      1125               16\n10 No             0.963    0.0368 No           44       477                7\n# ℹ 359 more rows\n# ℹ 13 more variables: HourlyRate &lt;int&gt;, JobLevel &lt;int&gt;, MonthlyIncome &lt;int&gt;,\n#   MonthlyRate &lt;int&gt;, NumCompaniesWorked &lt;int&gt;, PercentSalaryHike &lt;int&gt;,\n#   StockOptionLevel &lt;int&gt;, TotalWorkingYears &lt;int&gt;,\n#   TrainingTimesLastYear &lt;int&gt;, YearsAtCompany &lt;int&gt;,\n#   YearsInCurrentRole &lt;int&gt;, YearsSinceLastPromotion &lt;int&gt;,\n#   YearsWithCurrManager &lt;int&gt;"
  },
  {
    "objectID": "02-FE.html#customize-your-model-assessment",
    "href": "02-FE.html#customize-your-model-assessment",
    "title": "2  Transforming Features",
    "section": "2.4 Customize your model assessment",
    "text": "2.4 Customize your model assessment\nCreating custom assessment functions is quite convenient when iterating through various models. The metric_set() function from the yardstickpackage can help you to achieve this.\nDefine a function that returns roc_auc, accuracy, sens(sensitivity) and specificity spec(specificity) and use it to assess your model.\nThe augmented data frame lr_augis already loaded and ready to go."
  },
  {
    "objectID": "02-FE.html#instructions-2",
    "href": "02-FE.html#instructions-2",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nDefine a custom assessment function that returns roc_auc, accuracy, sens, and spec.\nAssess your model using your new function on lr_augto obtain the metrics you just chose.\n\n\n# Define a custom assessment function \nclass_evaluate &lt;- metric_set(roc_auc, accuracy, sens, spec)\n\n# Assess your model using your new function\nclass_evaluate(lr_aug, truth = Attrition,\n               estimate = .pred_class,\n               .pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n2 sens     binary         0.997\n3 spec     binary         0.05 \n4 roc_auc  binary         0.716"
  },
  {
    "objectID": "02-FE.html#common-feature-transformations--video",
    "href": "02-FE.html#common-feature-transformations--video",
    "title": "2  Transforming Features",
    "section": "Common feature transformations -(video)",
    "text": "Common feature transformations -(video)"
  },
  {
    "objectID": "02-FE.html#common-transformations",
    "href": "02-FE.html#common-transformations",
    "title": "2  Transforming Features",
    "section": "2.5 Common transformations",
    "text": "2.5 Common transformations\nThis transformation has no restrictions on the values it can take.\n\nBox-Cox\nYeo-Johnson\nBoth Yeo-Johnson and Box-Cox can take any values\nNeither. Both accept only positive values"
  },
  {
    "objectID": "02-FE.html#plain-recipe",
    "href": "02-FE.html#plain-recipe",
    "title": "2  Transforming Features",
    "section": "2.6 Plain recipe",
    "text": "2.6 Plain recipe\nUsing the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get started, you will define a plain recipe that does nothing other than define the model formula and the training data.\nThe attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits are loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-3",
    "href": "02-FE.html#instructions-3",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a plain recipe defining only the model formula.\n\n\n# Create a plain recipe defining only the model formula\nlr_recipe_plain &lt;- \n  recipe(Attrition ~., data = train)\n\nlr_workflow_plain &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_plain)\nlr_fit_plain &lt;- lr_workflow_plain %&gt;%\n  fit(train)\nlr_aug_plain &lt;-\n  lr_fit_plain %&gt;% augment(test)\nlr_aug_plain %&gt;% class_evaluate(truth = Attrition,\n                 estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n2 sens     binary         0.997\n3 spec     binary         0.05 \n4 roc_auc  binary         0.716"
  },
  {
    "objectID": "02-FE.html#box-cox-transformation",
    "href": "02-FE.html#box-cox-transformation",
    "title": "2  Transforming Features",
    "section": "2.7 Box-Cox transformation",
    "text": "2.7 Box-Cox transformation\nUsing the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get the features to behave nearly normally, you will create a recipe that implements the Box-Cox transformation.\nThe attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the train and test splits are loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-4",
    "href": "02-FE.html#instructions-4",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a recipe that uses Box-Cox to transform all numeric features, including the target.\n\n\n# Create a recipe that uses Box-Cox to transform all numeric features\nlr_recipe_BC &lt;- \n  recipe(Attrition ~., data = train) %&gt;%\n step_BoxCox(all_numeric())\n\nlr_workflow_BC &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_BC)\nlr_fit_BC &lt;- lr_workflow_BC %&gt;%\n  fit(train)\nlr_aug_BC &lt;-\n  lr_fit_BC %&gt;% augment(test)\nlr_aug_BC %&gt;% class_evaluate(truth = Attrition,\n                 estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.846 \n2 sens     binary        0.997 \n3 spec     binary        0.0667\n4 roc_auc  binary        0.722"
  },
  {
    "objectID": "02-FE.html#yeo-johnson-transformation",
    "href": "02-FE.html#yeo-johnson-transformation",
    "title": "2  Transforming Features",
    "section": "2.8 Yeo-Johnson transformation",
    "text": "2.8 Yeo-Johnson transformation\nUsing the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get the features to behave nearly normally, you will create a recipe that implements the Yeo-Johnson transformation.\nThe attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the train and test splits are loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-5",
    "href": "02-FE.html#instructions-5",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a recipe that uses Yeo-Johnson to transform all numeric features, including the target.\n\n\n# Create a recipe that uses Yeo-Johnson to transform all numeric features\nlr_recipe_YJ &lt;- \n  recipe(Attrition ~., data = train) %&gt;%\n  step_YeoJohnson(all_numeric())\n\nlr_workflow_YJ &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_YJ)\nlr_fit_YJ &lt;- lr_workflow_YJ %&gt;%\n  fit(train)\nlr_aug_YJ &lt;-\n  lr_fit_YJ %&gt;% augment(test)\nlr_aug_YJ %&gt;% class_evaluate(truth = Attrition,\n                 estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.846 \n2 sens     binary        0.994 \n3 spec     binary        0.0833\n4 roc_auc  binary        0.705"
  },
  {
    "objectID": "02-FE.html#advanced-transformations--video",
    "href": "02-FE.html#advanced-transformations--video",
    "title": "2  Transforming Features",
    "section": "Advanced transformations -(video)",
    "text": "Advanced transformations -(video)"
  },
  {
    "objectID": "02-FE.html#baseline",
    "href": "02-FE.html#baseline",
    "title": "2  Transforming Features",
    "section": "2.9 Baseline",
    "text": "2.9 Baseline\nContinuing with the attrition_num dataset, you will create a baseline with a plain recipe to assess the effects of additional feature engineering steps. The attrition_num data, the logistic regression lr_model, the user-defined class-evaluate() function, and the train and test splits have already been loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-6",
    "href": "02-FE.html#instructions-6",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nBundle the model and recipe into a workflow.\nAugment the fitted workflow to get it ready for assessment.\n\n\nlr_recipe_plain &lt;- recipe(Attrition ~., data = train)\n\n# Bundle the model and recipe\nlr_workflow_plain &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_plain)\nlr_fit_plain &lt;- lr_workflow_plain %&gt;%\n  fit(train)\n\n# Augment the fit workflow\nlr_aug_plain &lt;- lr_fit_plain %&gt;%\n  augment(test)\nlr_aug_plain %&gt;%\n  class_evaluate(truth = Attrition,estimate = .pred_class,\n                 .pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n2 sens     binary         0.997\n3 spec     binary         0.05 \n4 roc_auc  binary         0.716"
  },
  {
    "objectID": "02-FE.html#step_poly",
    "href": "02-FE.html#step_poly",
    "title": "2  Transforming Features",
    "section": "2.10 step_poly()",
    "text": "2.10 step_poly()\nNow that you have a baseline, you can compare your model’s performance if you add a polynomial transformation to all numerical values.\nThe attrition_num data, the logistic regression lr_model, the user-defined class-evaluate() function, and the train and test splits have already been loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-7",
    "href": "02-FE.html#instructions-7",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nAdd a polynomial transformation to all numeric predictors.\nFit workflow to the train data.\n\n\nlr_recipe_poly &lt;- \n  recipe(Attrition ~., data = train) %&gt;%\n\n# Add a polynomial transformation to all numeric predictors\n  step_poly(all_numeric_predictors())\n\nlr_workflow_poly &lt;- workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_poly)\n\n# Fit workflow to the train data\nlr_fit_poly &lt;- lr_workflow_poly %&gt;% fit(train)\nlr_aug_poly &lt;- lr_fit_poly %&gt;% augment(test)\nlr_aug_poly %&gt;% class_evaluate(truth = Attrition, estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.854\n2 sens     binary         0.990\n3 spec     binary         0.15 \n4 roc_auc  binary         0.742"
  },
  {
    "objectID": "02-FE.html#step_percentile",
    "href": "02-FE.html#step_percentile",
    "title": "2  Transforming Features",
    "section": "2.11 step_percentile()",
    "text": "2.11 step_percentile()\nHow would applying a percentile transformation to your numeric variables affect model performance? Try it!\nThe attrition_num data, the logistic regression lr_model, the user-defined class-evaluate() function, and the train and test splits have already been loaded for you."
  },
  {
    "objectID": "02-FE.html#instructions-8",
    "href": "02-FE.html#instructions-8",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nApply a percentile transformation to all numeric predictors.\n\n\n# Add percentile tansformation to all numeric predictors\nlr_recipe_perc &lt;- \n  recipe(Attrition ~., data = train) %&gt;%\n  step_percentile(all_numeric_predictors())\nlr_workflow_perc &lt;-\n  workflow() %&gt;%\n  add_model(lr_model) %&gt;%\n  add_recipe(lr_recipe_perc)\nlr_fit_perc &lt;- lr_workflow_perc %&gt;% fit(train)\nlr_aug_perc &lt;- lr_fit_perc %&gt;% augment(test)\nlr_aug_perc %&gt;% class_evaluate(truth = Attrition,\n                 estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.840\n2 sens     binary         0.994\n3 spec     binary         0.05 \n4 roc_auc  binary         0.723"
  },
  {
    "objectID": "02-FE.html#whos-staying",
    "href": "02-FE.html#whos-staying",
    "title": "2  Transforming Features",
    "section": "2.12 Who’s staying?",
    "text": "2.12 Who’s staying?\nIt’s time to practice combining several transformations to the attrition_num data. First, normalize or near-normalize numeric variables by applying a Yeo-Johnson transformation. Next, transform numeric predictors to percentiles, create dummy variables, and eliminate features with near zero variance."
  },
  {
    "objectID": "02-FE.html#instructions-9",
    "href": "02-FE.html#instructions-9",
    "title": "2  Transforming Features",
    "section": "Instructions",
    "text": "Instructions\n\nApply a Yeo-Johnson transformation to all numeric variables.\nTransform all numeric predictors into percentiles.\nCreate dummy variables for all nominal predictors.\n\n\nlr_recipe &lt;- recipe(Attrition ~., data = train) %&gt;%\n\n# Apply a Yeo-Johnson transformation to all numeric variables\n  step_YeoJohnson(all_numeric()) %&gt;%\n\n# Transform all numeric predictors into percentiles\n step_percentile(all_numeric_predictors()) %&gt;%\n\n# Create dummy variables for all nominal predictors\n  step_dummy(all_nominal_predictors())\n\nlr_workflow &lt;- workflow() %&gt;% add_model(lr_model) %&gt;% add_recipe(lr_recipe)\nlr_fit &lt;- lr_workflow %&gt;% fit(train)\nlr_aug &lt;- lr_fit %&gt;% augment(test)\nlr_aug %&gt;% class_evaluate(truth = Attrition, estimate = .pred_class,.pred_No)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.840\n2 sens     binary         0.994\n3 spec     binary         0.05 \n4 roc_auc  binary         0.723\n\n\n\n\n\n\n\n\nNote\n\n\n\nALLLLLL DONE"
  }
]